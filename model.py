# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXVF942SqcYOZzu7uCTNcahofvMu7_Ur
"""

#Connect to Google Drive
from google.colab import drive

drive.mount('/content/drive')

#Provides necessary tools to load the BERT
!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import torch.nn.functional as F
from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import accuracy_score,matthews_corrcoef

from tqdm import tqdm, trange,tnrange,tqdm_notebook
import random
import os
import io
# %matplotlib inline

# identify and specify the GPU as the device, later in training loop we will load data into device
#If cuda id available use gpu else use cpu
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#Get the number of gpu s available in this device
n_gpu = torch.cuda.device_count()
#Get the first available gpu
torch.cuda.get_device_name(0)

SEED = 19

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if device == torch.device("cuda"):
    torch.cuda.manual_seed_all(SEED)

#Pytorch is set to use the gpu
device = torch.device("cuda")

df_train = pd.read_csv("/content/drive/MyDrive/PSPD/Text & Emotion Dataset/train.txt", delimiter=';', header=None, names=['sentence','label'])
df_test = pd.read_csv("/content/drive/MyDrive/PSPD/Text & Emotion Dataset/test.txt", delimiter=';', header=None, names=['sentence','label'])
df_val = pd.read_csv("/content/drive/MyDrive/PSPD/Text & Emotion Dataset/val.txt", delimiter=';', header=None, names=['sentence','label'])

df = pd.concat([df_train,df_test,df_val])

df['label'].unique()

# Relabel other emotions to 'neutral'
df['label'] = df['label'].apply(lambda x: x if x in ['anger', 'sadness', 'joy'] else 'neutral')

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
df['label_enc'] = labelencoder.fit_transform(df['label'])

df[['label','label_enc']].drop_duplicates(keep='first')

df.rename(columns={'label':'label_desc'},inplace=True)
df.rename(columns={'label_enc':'label'},inplace=True)

## create label and sentence list
sentences = df.sentence.values

#check distribution of data based on labels
print("Distribution of data based on labels: ",df.label.value_counts())

# Set the maximum sequence length.
MAX_LEN = 256

## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)
input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, padding='max_length', truncation=True) for sent in sentences]

labels = df.label.values

print("Actual sentence before tokenization: ",sentences[2])
print("Encoded Input from dataset: ",input_ids[2])

## Create attention mask
attention_masks = []
## Create a mask of 1 for all input tokens and 0 for all padding tokens
attention_masks = [[float(i>0) for i in seq] for seq in input_ids]
print(attention_masks[2])

# class distribution
class_counts = [2709, 6761, 4733, 5797]

# Calculate the total number of samples
total = sum(class_counts)

# Calculate weights for each class
class_weights = [total/class_counts[i] for i in range(len(class_counts))]

# Convert class weights to a tensor
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

print("Class Weights:", class_weights)

# Define the loss function with class weights
loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)
train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)

# convert all our data into torch tensors, required data type for our model
train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)
train_labels = torch.tensor(train_labels)
validation_labels = torch.tensor(validation_labels)
train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

# Select a batch size for training.
batch_size = 32

# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,
# with an iterator the entire dataset does not need to be loaded into memory
train_data = TensorDataset(train_inputs,train_masks,train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)

validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)
validation_sampler = RandomSampler(validation_data)
validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)

train_data[0]

type(train_dataloader)

from transformers import BertForSequenceClassification, BertModel
import torch.nn as nn

class CustomBertModel(nn.Module):
    def __init__(self, num_labels=4):
        super(CustomBertModel, self).__init__()
        self.num_labels = num_labels
        self.bert = BertModel.from_pretrained("bert-base-uncased")

        # Custom layers
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Sequential(
            nn.Linear(768, 512),  # First additional layer
            nn.ReLU(),            #Rectified Linear Unit. Type of activation function
            nn.Dropout(0.1),     # Additional dropout layer
            nn.Linear(512, num_labels)  # Final classification layer
        )

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)

        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

# Initialize the custom model
model = CustomBertModel(num_labels=4).to(device)

# Parameters:
lr = 2e-5
adam_epsilon = 1e-8

# Number of training epochs
epochs = 3

num_warmup_steps = 0
num_training_steps = len(train_dataloader)*epochs

optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler

## Store our loss and accuracy for plotting
train_loss_set = []
learning_rate = []

# Gradients gets accumulated by default
model.zero_grad()

# tnrange is a tqdm wrapper around the normal python range
for _ in tnrange(1, epochs+1, desc='Epoch'):
    print("<" + "="*22 + F" Epoch {_} "+ "="*22 + ">")
    # Calculate total loss for this epoch
    batch_loss = 0

    for step, batch in enumerate(train_dataloader):
        # Set our model to training mode (as opposed to evaluation mode)
        model.train()

        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch

        # Forward pass
        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)
        logits = outputs
        loss = loss_fn(logits.view(-1, 4), b_labels.view(-1))

        # Backward pass
        loss.backward()

        # Clip the norm of the gradients to 1.0
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient
        optimizer.step()

        # Update learning rate schedule
        scheduler.step()

        # Clear the previous accumulated gradients
        optimizer.zero_grad()

        # Update tracking variables
        batch_loss += loss.item()

    # Calculate the average loss over the training data.
    avg_train_loss = batch_loss / len(train_dataloader)

    # Store the current learning rate
    for param_group in optimizer.param_groups:
        print("\n\tCurrent Learning rate: ", param_group['lr'])
        learning_rate.append(param_group['lr'])

    train_loss_set.append(avg_train_loss)
    print(F'\n\tAverage Training loss: {avg_train_loss}')

    # Validation

    # Put model in evaluation mode to evaluate loss on the validation set
    model.eval()
    df_metrics = pd.DataFrame(columns=['Actual_class', 'Predicted_class'])

    # Tracking variables
    eval_accuracy, eval_mcc_accuracy, nb_eval_steps = 0, 0, 0

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch

        # Telling the model not to compute or store gradients, saving memory and speeding up validation
        with torch.no_grad():
            # Forward pass, calculate logit predictions
            outputs = model(b_input_ids, attention_mask=b_input_mask)
            logits = outputs if isinstance(outputs, tuple) else outputs

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        pred_flat = np.argmax(logits, axis=1).flatten()
        labels_flat = label_ids.flatten()

        batch_results = pd.DataFrame({
            'Actual_class': labels_flat,
            'Predicted_class': pred_flat
        })

        df_metrics = pd.concat([df_metrics, batch_results], ignore_index=True)

        tmp_eval_accuracy = accuracy_score(labels_flat, pred_flat)
        tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)

        eval_accuracy += tmp_eval_accuracy
        eval_mcc_accuracy += tmp_eval_mcc_accuracy
        nb_eval_steps += 1

    print(F'\n\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')
    print(F'\n\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')

from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting normalize=True.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

df[['label','label_desc']].drop_duplicates(keep='first')

## emotion labels
label2int = {
  "sadness": 3,
  "joy": 1,
  "anger": 0,
  "neutral": 2
}



df_metrics['Predicted_class'].unique()

print(df_metrics['Actual_class'].unique())
print(df_metrics['Predicted_class'].unique())
print(df_metrics['Actual_class'].isna().any())
print(df_metrics['Predicted_class'].isna().any())

print(df_metrics['Actual_class'].dtype)
print(df_metrics['Predicted_class'].dtype)

df_metrics['Actual_class'] = df_metrics['Actual_class'].astype(int)
df_metrics['Predicted_class'] = df_metrics['Predicted_class'].astype(int)

from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming your 'plot_confusion_matrix' function is defined elsewhere in the script as you've shown

# Calculate the confusion matrix
cm = confusion_matrix(df_metrics['Actual_class'], df_metrics['Predicted_class'])

# Extract class names from your label2int mapping
class_names = list(label2int.keys())

# Plot the confusion matrix using your custom plotting function
plot_confusion_matrix(cm, classes=class_names, normalize=True,
                      title='Normalized Confusion Matrix')

plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate confusion matrix
cm = confusion_matrix(df_metrics['Actual_class'], df_metrics['Predicted_class'])

# Plot confusion matrix
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label2int.keys(), yticklabels=label2int.keys())
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Assuming 'model' is your trained model and 'validation_dataloader' is your DataLoader
model.eval()  # Put the model in evaluation mode

predicted_probs = []  # List to store probabilities

for batch in validation_dataloader:
    batch = tuple(t.to(device) for t in batch)
    b_input_ids, b_input_mask, _ = batch

    with torch.no_grad():  # Inference without calculating gradients
        outputs = model(b_input_ids, attention_mask=b_input_mask)

    logits = outputs
    probs = torch.nn.functional.softmax(logits, dim=1)  # Apply softmax to convert logits to probabilities

    predicted_probs.extend(probs.cpu().numpy())  # Move probs to CPU and convert to numpy

# Convert the list of arrays into a single numpy array
predicted_probs = np.array(predicted_probs)

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Binarize the actual labels for the multi-class case
y_bin = label_binarize(df_metrics['Actual_class'], classes=range(len(label2int)))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(len(label2int)):
    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], predicted_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot all ROC curves
plt.figure()
colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])
for i, color in zip(range(len(label2int)), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(list(label2int.keys())[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC Curve')
plt.legend(loc="lower right")
plt.show()

y_true = df_metrics['Actual_class'].astype(int)  # Ensure integer type
y_pred = df_metrics['Predicted_class'].astype(int)

# Calculate metrics as shown above
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='macro')
recall = recall_score(y_true, y_pred, average='macro')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")

from sklearn.metrics import accuracy_score, precision_score, recall_score

# Calculate accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy}")

# Calculate precision
precision = precision_score(y_true, y_pred, average='weighted')
print(f"Precision: {precision}")

# Calculate recall
recall = recall_score(y_true, y_pred, average='weighted')
print(f"Recall: {recall}")

report = classification_report(y_true, y_pred, target_names=label2int.keys())

# Print the classification report
print(report)

torch.save(model.state_dict(), "/content/drive/MyDrive/PSPD/Custom-Model.pth")

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess(text):
    # Tokenize the text with the same parameters used during training
    inputs = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=256,
        padding='max_length',  # Updated padding
        return_attention_mask=True,
        return_tensors='pt', # Return PyTorch tensors
        truncation=True
    )
    return inputs['input_ids'], inputs['attention_mask']

# Sample text
sample_text = "I feel great today!"
input_ids, attention_mask = preprocess(sample_text)

model.eval()

# Move the model to the right device (if using GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#  move input tensors to the same device
input_ids = input_ids.to(device)
attention_mask = attention_mask.to(device)

with torch.no_grad():
    logits = model(input_ids, attention_mask=attention_mask)
    probabilities = torch.nn.functional.softmax(logits, dim=1)
    predicted_class = torch.argmax(probabilities, dim=1)

# Move predicted_class back to CPU for further operations if necessary
predicted_class = predicted_class.cpu()

print("Predicted class:", predicted_class.item())

# Inverse mapping from integers to labels
int2label = {v: k for k, v in label2int.items()}


model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

input_ids = input_ids.to(device)
attention_mask = attention_mask.to(device)

with torch.no_grad():
    logits = model(input_ids, attention_mask=attention_mask)
    probabilities = torch.nn.functional.softmax(logits, dim=1)
    predicted_class_index = torch.argmax(probabilities, dim=1)
    predicted_class_index = predicted_class_index.cpu()

# Use the inverse mapping to get the predicted label
predicted_label = int2label[predicted_class_index.item()]

print("Predicted emotion:", predicted_label)

# Load the dataset
dataset_path = '/content/drive/MyDrive/PSPD/summary book.csv'
df = pd.read_csv(dataset_path)

import pandas as pd
import re
# Remove rows with NaN descriptions
df.dropna(subset=['book_desc'], inplace=True)

# Remove rows with descriptions less than 10 words
df = df[df['book_desc'].apply(lambda x: len(str(x).split()) >= 10)]

# Define a regex pattern for odd symbols (adjust the pattern to match odd symbols you want to remove)
odd_symbols_pattern = r'[^a-zA-Z0-9 .,!?\'"-]'

# Keep rows without odd symbols in descriptions
df = df[df['book_desc'].apply(lambda x: not bool(re.search(odd_symbols_pattern, str(x))))]
columns_to_remove = ['book_authors', 'book_edition', 'book_format', 'book_pages', 'book_rating', 'book_rating_count', 'book_review_count', 'book_title', 'genres', 'image_url','book_title']
df = df.drop(columns=columns_to_remove)

import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
import pandas as pd

class CustomBertModel(nn.Module):
    def __init__(self, num_labels=4):
        super(CustomBertModel, self).__init__()
        self.num_labels = num_labels
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, num_labels)
        )

    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

# Load the model
model_path = "/content/drive/MyDrive/PSPD/Custom-Model.pth"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CustomBertModel(num_labels=4)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()
model.to(device)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_summaries(summaries, max_length=512):
    return tokenizer(summaries, padding=True, truncation=True, max_length=max_length, return_tensors="pt")

# Define the label mapping based on model's training
int2label = {0: 'anger', 1: 'joy', 2: 'neutral', 3: 'sadness'}

def predict_emotion(summary):
    tokenized_inputs = tokenize_summaries([summary])
    input_ids = tokenized_inputs["input_ids"].to(device)
    attention_mask = tokenized_inputs["attention_mask"].to(device)

    with torch.no_grad():
        logits = model(input_ids, attention_mask=attention_mask)
        probabilities = torch.nn.functional.softmax(logits, dim=1)
        predicted_class_index = torch.argmax(probabilities, dim=1)
        predicted_label = int2label[predicted_class_index.item()]

    return predicted_label

df['predicted_emotion'] = df['book_desc'].apply(predict_emotion)


print(df.head()) # Just to verify the data is loaded correctly
output_path = "/content/drive/MyDrive/PSPD/Book-Emotion.csv"
df.to_csv(output_path, index=False)

df.drop('book_desc', axis=1, inplace=True)
df.rename(columns={'book_isbn': 'ISBN'}, inplace=True)
df.rename(columns={'predicted_emotion': 'Emotion'}, inplace=True)

file_path = '/content/drive/MyDrive/PSPD/Context-Data.csv'#Path to context data

# Read the CSV file into a Pandas DataFrame
df_context= pd.read_csv(file_path)

import pandas as pd

# Merge the datasets on ISBN
merged_df = pd.merge(df_context,df, on='ISBN', how='left')

merged_df['Emotion'].fillna('unknown', inplace=True)
# Define the mapping from old to new values
emotion_mapping = {
    'sadness': 'sad',
    'joy': 'happy',
    'anger': 'angry',
    # 'neutral' remains the same
}

# Apply the mapping to the 'emotion' column
merged_df['Emotion'] = df['Emotion'].replace(emotion_mapping)

# Save the merged dataset to a new CSV file
merged_df.to_csv('/content/drive/MyDrive/PSPD/FinalFiltering.csv', index=False)

file_path = '/content/drive/MyDrive/PSPD/FinalFiltering.csv'
# Read the CSV file into a Pandas DataFrame
df_book_final= pd.read_csv(file_path)

!pip install fer
from fer import FER
import matplotlib.pyplot as plt
import cv2

# Load an image
image_path = '/content/drive/MyDrive/PSPD/justin.jpg'
img = cv2.imread(image_path)

# Convert BGR (OpenCV format) to RGB (matplotlib format)
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Initialize the FER object
detector = FER()

# Detect emotions in the image
emotions = detector.detect_emotions(img_rgb)
if emotions:
    # Get the dominant emotion
    dominant_emotion = max(emotions[0]['emotions'], key=emotions[0]['emotions'].get)
    print("Dominant Emotion:", dominant_emotion)

    #Display the image with the detected dominant emotion
    plt.imshow(img_rgb)
    plt.title(f"Dominant Emotion: {dominant_emotion}")
    plt.axis('off')  # Turn off axis numbers and ticks
    plt.show()
else:
    print("No emotions detected.")

age = input("Please enter your age: ")

location = input("Please enter your location: ")

from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Define the transformers for the column transformer
user_categorical_cols = ['Location', 'Emotion']
user_numerical_cols = ['Age']

# Transformer for user attributes
user_preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), user_numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), user_categorical_cols)
    ])

# User input example
user_input = {'Age': age, 'Location': location, 'Emotion': dominant_emotion}
user_input_df = pd.DataFrame([user_input])
user_input_transformed = user_preprocessor.fit_transform(user_input_df)

# Extract book features and ratings
book_features = df_book_final[['Age', 'Location', 'Emotion']]
book_ratings = df_book_final['Book-Rating'].values.reshape(-1, 1)

# Apply transformations to book features
book_features_transformed = user_preprocessor.transform(book_features)

# Normalize book ratings
scaler = MinMaxScaler()
normalized_ratings = scaler.fit_transform(book_ratings)

# Compute similarity
similarity_scores = cosine_similarity(user_input_transformed, book_features_transformed)

# Adjust similarity scores by book ratings
adjusted_scores = similarity_scores.flatten() * normalized_ratings.flatten()

# Get top 5 similar book titles
top_5_indices = adjusted_scores.argsort()[-5:][::-1]
top_5_books = df_book_final['Book-Title'].iloc[top_5_indices]

print("Recommended Books:", top_5_books.tolist())